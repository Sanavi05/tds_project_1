Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Embeddings: OpenAI and Local Models
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Correlation with Excel
Regression with Excel
Forecasting with Excel
Outlier Detection with Excel
Data Analysis with Python
Data Analysis with SQL
Data Analysis with Datasette
Data Analysis with DuckDB
Data Analysis with ChatGPT
Geospatial Analysis with Excel
Geospatial Analysis with Python
Geospatial Analysis with QGIS
Network Analysis in Python
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
Embeddings: OpenAI and Local Models
Embedding models convert text into a list of numbers. These are like a map of text in numerical form. Each number represents a feature, and similar texts will have numbers close to each other. So, if the numbers are similar, the text they represent mean something similar.
This is useful because text similarity is important in many common problems:
Search
. Find similar documents to a query.
Classification
. Classify text into categories.
Clustering
. Group similar items into clusters.
Anomaly Detection
. Find an unusual piece of text.
You can run embedding models locally or using an API. Local models are better for privacy and cost. APIs are better for scale and quality.
Feature
Local Models
API
Privacy
High
Dependent on provider
Cost
High setup, low after that
Pay-as-you-go
Scale
Limited by local resources
Easily scales with demand
Quality
Varies by model
Typically high
The
Massive Text Embedding Benchmark (MTEB)
provides comprehensive comparisons of embedding models. These models are compared on several parameters, but here are some key ones to look at:
Rank
. Higher ranked models have higher quality.
Memory Usage
. Lower is better (for similar ranks). It costs less and is faster to run.
Embedding Dimensions
. Lower is better. This is the number of numbers in the array. Smaller dimensions are cheaper to store.
Max Tokens
. Higher is better. This is the number of input tokens (words) the model can take in a
single
input.
Look for higher scores in the columns for Classification, Clustering, Summarization, etc. based on your needs.
Local Embeddings
Here’s a minimal example using a local embedding model:
# /// script
# requires-python = "==3.12"
# dependencies = [
#   "sentence-transformers",
#   "httpx",
#   "numpy",
# ]
# ///
from
sentence_transformers
import
SentenceTransformer
import
numpy
as
np

model
=
SentenceTransformer
(
'BAAI/bge-base-en-v1.5'
)
# A small, high quality model
async
def
embed
(
text
:
str
)
-
>
list
[
float
]
:
"""Get embedding vector for text using local model."""
return
model
.
encode
(
text
)
.
tolist
(
)
async
def
get_similarity
(
text1
:
str
,
text2
:
str
)
-
>
float
:
"""Calculate cosine similarity between two texts."""
emb1
=
np
.
array
(
await
embed
(
text1
)
)
emb2
=
np
.
array
(
await
embed
(
text2
)
)
return
float
(
np
.
dot
(
emb1
,
emb2
)
/
(
np
.
linalg
.
norm
(
emb1
)
*
np
.
linalg
.
norm
(
emb2
)
)
)
async
def
main
(
)
:
print
(
await
get_similarity
(
"Apple"
,
"Orange"
)
)
print
(
await
get_similarity
(
"Apple"
,
"Lightning"
)
)
if
__name__
==
"__main__"
:
import
asyncio
    asyncio
.
run
(
main
(
)
)
Copy to clipboard
Error
Copied
Note the
get_similarity
function. It uses a
Cosine Similarity
to calculate the similarity between two embeddings.
OpenAI Embeddings
For comparison, here’s how to use OpenAI’s API with direct HTTP calls. Replace the
embed
function in the earlier script:
import
os
import
httpx
async
def
embed
(
text
:
str
)
-
>
list
[
float
]
:
"""Get embedding vector for text using OpenAI's API."""
async
with
httpx
.
AsyncClient
(
)
as
client
:
response
=
await
client
.
post
(
"https://api.openai.com/v1/embeddings"
,
headers
=
{
"Authorization"
:
f"Bearer
{
os
.
environ
[
'OPENAI_API_KEY'
]
}
"
}
,
json
=
{
"model"
:
"text-embedding-3-small"
,
"input"
:
text
}
)
return
response
.
json
(
)
[
"data"
]
[
0
]
[
"embedding"
]
Copy to clipboard
Error
Copied
NOTE
: You need to set the
OPENAI_API_KEY
environment variable for this to work.
Previous
Vision Models
Next
Multimodal Embeddings