Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Correlation with Excel
Regression with Excel
Forecasting with Excel
Outlier Detection with Excel
Data Analysis with Python
Data Analysis with SQL
Data Analysis with Datasette
Data Analysis with DuckDB
Data Analysis with DuckDB
Data Analysis with ChatGPT
Geospatial Analysis with Excel
Geospatial Analysis with Python
Geospatial Analysis with QGIS
Network Analysis in Python
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
Data Analysis with DuckDB
You’ll learn how to perform data analysis using DuckDB and Pandas, covering:
Parquet for Data Storage
: Understand why Parquet is a faster, more compact, and better-typed storage format compared to CSV, JSON, and SQLite.
DuckDB Setup
: Learn how to install and set up DuckDB, along with integrating it into a Jupyter notebook environment.
File Format Comparisons
: Compare file formats by speed and size, observing the performance difference between saving and loading data in CSV, JSON, SQLite, and Parquet.
Faster Queries with DuckDB
: Learn how DuckDB uses parallel processing, columnar storage, and on-disk operations to outperform Pandas in speed and memory efficiency.
SQL Query Execution in DuckDB
: Run SQL queries directly on Parquet files and Pandas DataFrames to compute metrics such as the number of unique flight routes delayed by certain time intervals.
Memory Efficiency
: Understand how DuckDB performs analytics without loading entire datasets into memory, making it highly efficient for large-scale data analysis.
Mixing DuckDB and Pandas
: Learn to interleave DuckDB and Pandas operations, leveraging the strengths of both tools to perform complex queries like correlations and aggregations.
Ranking and Filtering Data
: Use SQL and Pandas to rank arrival delays by distance and extract key insights, such as the earliest flight arrival for each route.
Joining Data
: Create a cost analysis by joining datasets and calculating total costs of flight delays, demonstrating DuckDB’s speed in joining and aggregating large datasets.
Here are the links used in the video:
Data analysis with DuckDB - Notebook
Parquet file format
- a fast columnar storage format that’s becoming a de facto standard for big data
DuckDB
- a fast in-memory database that’s very good with large-scale analysis
Plotly Datasets
- a collection of sample datasets for analysis. This includes the
Kaggle Flights Dataset
that the notebook downloads as
2015_flights.parquet
Previous
Data Analysis with Datasette
Next
Data Analysis with ChatGPT