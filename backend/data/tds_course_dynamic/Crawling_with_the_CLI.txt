Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Correlation with Excel
Regression with Excel
Forecasting with Excel
Outlier Detection with Excel
Data Analysis with Python
Data Analysis with SQL
Data Analysis with Datasette
Data Analysis with DuckDB
Data Analysis with ChatGPT
Geospatial Analysis with Excel
Geospatial Analysis with Python
Geospatial Analysis with QGIS
Network Analysis in Python
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
Crawling with the CLI
Since websites are a common source of data, we often download entire websites (crawling) and then process them offline.
Web crawling is essential in many data-driven scenarios:
Data mining and analysis
: Gathering structured data from multiple pages for market research, competitive analysis, or academic research
Content archiving
: Creating offline copies of websites for preservation or backup purposes
SEO analysis
: Analyzing site structure, metadata, and content to improve search rankings
Legal compliance
: Capturing website content for regulatory or compliance documentation
Website migration
: Creating a complete copy before moving to a new platform or design
Offline access
: Downloading educational resources, documentation, or reference materials for use without internet connection
The most commonly used tool for fetching websites is
wget
. It is pre-installed in many UNIX distributions and easy to install.
To crawl the
IIT Madras Data Science Program website
for example, you could run:
wget
\
--recursive
\
--level
=
3
\
--no-parent
\
--convert-links
\
--adjust-extension
\
--compression
=
auto
\
--accept
html,htm
\
--directory-prefix
=
./ds
\
https://study.iitm.ac.in/ds/
Copy to clipboard
Error
Copied
Here’s what each option does:
--recursive
: Enables recursive downloading (following links)
--level=3
: Limits recursion depth to 3 levels from the initial URL
--no-parent
: Restricts crawling to only URLs below the initial directory
--convert-links
: Converts all links in downloaded documents to work locally
--adjust-extension
: Adds proper extensions to files (.html, .jpg, etc.) based on MIME types
--compression=auto
: Automatically handles compressed content (gzip, deflate)
--accept html,htm
: Only downloads files with these extensions
--directory-prefix=./ds
: Saves all downloaded files to the specified directory
wget2
is a better version of
wget
and supports HTTP2, parallel connections, and only updates modified sites. The syntax is (mostly) the same.
wget2
\
--recursive
\
--level
=
3
\
--no-parent
\
--convert-links
\
--adjust-extension
\
--compression
=
auto
\
--accept
html,htm
\
--directory-prefix
=
./ds
\
https://study.iitm.ac.in/ds/
Copy to clipboard
Error
Copied
There are popular free and open-source alternatives to Wget:
Wpull
Wpull
is a wget‐compatible Python crawler that supports on-disk resumption, WARC output, and PhantomJS integration.
uvx wpull
\
--recursive
\
--level
=
3
\
--no-parent
\
--convert-links
\
--adjust-extension
\
--compression
=
auto
\
--accept
html,htm
\
--directory-prefix
=
./ds
\
https://study.iitm.ac.in/ds/
Copy to clipboard
Error
Copied
HTTrack
HTTrack
is dedicated website‐mirroring tool with rich filtering and link‐conversion options.
httrack
"https://study.iitm.ac.in/ds/"
\
-O
"./ds"
\
"+*.study.iitm.ac.in/ds/*"
\
-r3
Copy to clipboard
Error
Copied
Robots.txt
robots.txt
is a standard file found in a website’s root directory that specifies which parts of the site should not be accessed by web crawlers. It’s part of the Robots Exclusion Protocol, an ethical standard for web crawling.
Why it’s important
:
Server load protection
: Prevents excessive traffic that could overload servers
Privacy protection
: Keeps sensitive or private content from being indexed
Legal compliance
: Respects website owners’ rights to control access to their content
Ethical web citizenship
: Shows respect for website administrators’ wishes
How to override robots.txt restrictions
:
wget, wget2
: Use
-e robots=off
httrack
: Use
-s0
wpull
: Use
--no-robots
When to override robots.txt (use with discretion)
:
Only bypass
robots.txt
when:
You have explicit permission from the website owner
You’re crawling your own website
The content is publicly accessible and your crawling won’t cause server issues
You’re conducting authorized security testing
Remember that bypassing
robots.txt
without legitimate reason may:
Violate terms of service
Lead to IP banning
Result in legal consequences in some jurisdictions
Cause reputation damage to your organization
Always use the minimum necessary crawling speed and scope, and consider contacting website administrators for permission when in doubt.
Previous
Scraping with Google Sheets
Next
BBC Weather API with Python